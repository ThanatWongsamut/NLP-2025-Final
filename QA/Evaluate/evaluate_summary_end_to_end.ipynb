{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "print(\"Initializing models...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "encoder = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')\n",
    "rouge_calculator = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.squeeze(0)  # shape: (seq_len, hidden_dim)\n",
    "\n",
    "def bert_similarity_matrix(ref_emb, cand_emb):\n",
    "    ref_norm = F.normalize(ref_emb, dim=1)\n",
    "    cand_norm = F.normalize(cand_emb, dim=1)\n",
    "    return ref_norm @ cand_norm.T  # shape: (len_ref, len_cand)\n",
    "\n",
    "def bert_score(text_ref, text_cand, tokenizer, model):\n",
    "    ref_emb = bert_encode(text_ref, tokenizer, model)\n",
    "    cand_emb = bert_encode(text_cand, tokenizer, model)\n",
    "    \n",
    "    sim_matrix = bert_similarity_matrix(ref_emb, cand_emb)\n",
    "\n",
    "    # Recall: average over reference tokens (rows)\n",
    "    recall = sim_matrix.max(dim=1).values.mean().item()\n",
    "\n",
    "    # Precision: average over candidate tokens (columns)\n",
    "    precision = sim_matrix.max(dim=0).values.mean().item()\n",
    "\n",
    "    # F1 score\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def pairwise_sim(ref_emb, cand_emb, scale=False):\n",
    "    ref_norm = F.normalize(ref_emb, dim=1)\n",
    "    cand_norm = F.normalize(cand_emb, dim=1) \n",
    "    sim_score = (ref_norm * cand_norm).sum(dim=1)  # shape: (n,), -1 to 1\n",
    "    if scale: \n",
    "        sim_score = (sim_score + 1) / 2  # scale to 0 to 1\n",
    "    return sim_score\n",
    "\n",
    "def calculate_rouge_metrics(references, candidates):\n",
    "    rouge1_precision = []\n",
    "    rouge1_recall = []\n",
    "    rouge1_f1 = []\n",
    "    \n",
    "    rouge2_precision = []\n",
    "    rouge2_recall = []\n",
    "    rouge2_f1 = []\n",
    "    \n",
    "    rougeL_precision = []\n",
    "    rougeL_recall = []\n",
    "    rougeL_f1 = []\n",
    "    \n",
    "    for ref, cand in zip(references, candidates):\n",
    "        scores = rouge_calculator.score(ref, cand)\n",
    "        \n",
    "        rouge1_precision.append(scores['rouge1'].precision)\n",
    "        rouge1_recall.append(scores['rouge1'].recall)\n",
    "        rouge1_f1.append(scores['rouge1'].fmeasure)\n",
    "        \n",
    "        rouge2_precision.append(scores['rouge2'].precision)\n",
    "        rouge2_recall.append(scores['rouge2'].recall)\n",
    "        rouge2_f1.append(scores['rouge2'].fmeasure)\n",
    "        \n",
    "        rougeL_precision.append(scores['rougeL'].precision)\n",
    "        rougeL_recall.append(scores['rougeL'].recall)\n",
    "        rougeL_f1.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1_precision': np.mean(rouge1_precision),\n",
    "        'rouge1_recall': np.mean(rouge1_recall),\n",
    "        'rouge1_f1': np.mean(rouge1_f1),\n",
    "        'rouge2_precision': np.mean(rouge2_precision),\n",
    "        'rouge2_recall': np.mean(rouge2_recall),\n",
    "        'rouge2_f1': np.mean(rouge2_f1),\n",
    "        'rougeL_precision': np.mean(rougeL_precision),\n",
    "        'rougeL_recall': np.mean(rougeL_recall),\n",
    "        'rougeL_f1': np.mean(rougeL_f1)\n",
    "    }\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate BERTScore\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing BERTScore\"):\n",
    "        ref = row['ground_truth_answer']\n",
    "        cand = row['predicted_answer']\n",
    "        score = bert_score(ref, cand, tokenizer, model)\n",
    "        df.at[i, 'precision'] = score['precision']\n",
    "        df.at[i, 'recall'] = score['recall']\n",
    "        df.at[i, 'f1'] = score['f1']\n",
    "    \n",
    "    # Extract data\n",
    "    queries = df['query'].tolist()\n",
    "    references = df['ground_truth_answer'].tolist()\n",
    "    candidates = df['predicted_answer'].tolist()\n",
    "    \n",
    "    # Compute encodings\n",
    "    print(\"Computing sentence embeddings...\")\n",
    "    q_embs = encoder.encode(queries, convert_to_tensor=True)\n",
    "    ref_embs = encoder.encode(references, convert_to_tensor=True)\n",
    "    cand_embs = encoder.encode(candidates, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    q_ref_sim = pairwise_sim(q_embs, ref_embs)\n",
    "    q_cand_sim = pairwise_sim(q_embs, cand_embs)\n",
    "    ref_cand_sim = pairwise_sim(ref_embs, cand_embs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metric1 = ref_cand_sim                                   # reference-candidate similarity\n",
    "    metric2 = q_cand_sim                                     # query-candidate relevance\n",
    "    \n",
    "    # Handle potential division by zero or very small values\n",
    "    safe_divisor = torch.where(q_ref_sim != 0, q_ref_sim, torch.ones_like(q_ref_sim) * 1e-8)\n",
    "    aggregated_metric = metric1 * q_cand_sim / safe_divisor   # reference-candidate similarity weighted by query-candidate relevance\n",
    "    \n",
    "    # Convert to numpy for calculations\n",
    "    metric1_np = metric1.cpu().numpy()\n",
    "    metric2_np = metric2.cpu().numpy()\n",
    "    aggregated_metric_np = aggregated_metric.cpu().numpy()\n",
    "    \n",
    "    # Calculate ROUGE metrics\n",
    "    print(\"Computing ROUGE metrics...\")\n",
    "    rouge_metrics = calculate_rouge_metrics(references, candidates)\n",
    "    \n",
    "    # Return all metrics\n",
    "    result = {\n",
    "        'file_name': os.path.basename(file_path),\n",
    "        'bertscore_precision': df['precision'].mean(),\n",
    "        'bertscore_recall': df['recall'].mean(),\n",
    "        'bertscore_f1': df['f1'].mean(),\n",
    "        'ref_cand_sim': metric1_np.mean(),\n",
    "        'query_cand_relevance': metric2_np.mean(),\n",
    "        'aggregated_metric': aggregated_metric_np.mean(),\n",
    "        **rouge_metrics\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./predictions_end_to_end/test_end_to_end_gemma3:12b.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing BERTScore: 100%|██████████| 244/244 [00:18<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sentence embeddings...\n",
      "Computing ROUGE metrics...\n",
      "Evaluation summary report written to evaluation_end_to_end_summary_report.txt\n",
      "CSV report written to evaluation_end_to_end_summary_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Find all CSV files in the ./predictions directory\n",
    "prediction_dir = './predictions_end_to_end/'\n",
    "csv_files = [os.path.join(prediction_dir, f) for f in os.listdir(prediction_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Process each file and collect results\n",
    "results = []\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        result = process_file(file_path)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Generate summary report\n",
    "report_path = \"evaluation_end_to_end_summary_report.txt\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"EVALUATION SUMMARY REPORT\\n\")\n",
    "    f.write(\"=======================\\n\\n\")\n",
    "    \n",
    "    for result in results:\n",
    "        f.write(f\"MODEL: {'_'.join(result['file_name'].split('_')[1:])}\\n\")\n",
    "        f.write(\"=\" * (len(\"MODEL: \") + len(result['file_name'])) + \"\\n\")\n",
    "        \n",
    "        f.write(\"   BERTScore:\\n\")\n",
    "        f.write(f\"      Precision: {result['bertscore_precision']:.4f}\\n\")\n",
    "        f.write(f\"      Recall: {result['bertscore_recall']:.4f}\\n\")\n",
    "        f.write(f\"      F1: {result['bertscore_f1']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"   Reference-Candidate Similarity:\\n\")\n",
    "        f.write(f\"      Mean: {result['ref_cand_sim']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"   Query-Candidate Relevance:\\n\")\n",
    "        f.write(f\"      Mean: {result['query_cand_relevance']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"   Aggregated Metric:\\n\")\n",
    "        f.write(f\"      Mean: {result['aggregated_metric']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"   ROUGE Metrics:\\n\")\n",
    "        f.write(f\"      ROUGE-1 Precision: {result['rouge1_precision']:.4f}\\n\")\n",
    "        f.write(f\"      ROUGE-1 Recall: {result['rouge1_recall']:.4f}\\n\")\n",
    "        f.write(f\"      ROUGE-1 F1: {result['rouge1_f1']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"      ROUGE-2 Precision: {result['rouge2_precision']:.4f}\\n\")\n",
    "        f.write(f\"      ROUGE-2 Recall: {result['rouge2_recall']:.4f}\\n\")\n",
    "        f.write(f\"      ROUGE-2 F1: {result['rouge2_f1']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"      ROUGE-L Precision: {result['rougeL_precision']:.4f}\\n\")\n",
    "        f.write(f\"      ROUGE-L Recall: {result['rougeL_recall']:.4f}\\n\")\n",
    "        f.write(f\"      ROUGE-L F1: {result['rougeL_f1']:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "# Also save as CSV for easier data analysis\n",
    "pd.DataFrame(results).to_csv(\"evaluation_end_to_end_summary_report.csv\", index=False)\n",
    "\n",
    "print(f\"Evaluation summary report written to {report_path}\")\n",
    "print(f\"CSV report written to evaluation_end_to_end_summary_report.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
